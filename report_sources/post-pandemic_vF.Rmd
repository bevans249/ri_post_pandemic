---
title: "Investigating post-pandemic global and country-level routine immunisation coverage trends"
author: "Beth Evans"
date: "28 March 2025"
output: 
  html_document:
    code_folding: "show"
    toc: TRUE
    toc_depth: 4
    toc_float: TRUE
    toc_collapse: FALSE
    number_sections: TRUE
    highlight: pygments
    theme: spacelab
params:
  data: "dtp3" # Defines which vaccine to run (DTP1 or DTP3)
  test_year: "2023"   # Defines which year to run for single year analyses (2020-2023) 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      dev = c("png", "pdf"),
                      fig.path = "figs/",
                      dpi = 100)
```

# GENERAL SET-UP

## Load packages

```{r, message = FALSE, include = FALSE}
library(epiDisplay)
library(tidyverse)
library(rio)
library(magrittr)
library(timetk)
library(broom)
library(scales)
library(ggforce)
library(urca)
library(tseries)
library(forecast)
library(data.table)
library(countrycode)
library(here)
library(flextable)
library(webshot)
library(wesanderson)
library(cowplot)
library(formattable)
library(spData)
library(rnaturalearth)
library(officer)
library(weights)
library(randomForest)
library(corrplot)
library(ggcorrplot)
library(MASS)
library(pROC)
library(caret)
library(adegenet)

select <- dplyr::select
```

## Load raw data for descriptive analyses
Import data:

- Coverage data from WUENIC for 2000-2022
- Income group classification from World Bank
- Key demographic data from United Nations World Population Prospects (UN WPP)

```{r}
# Set-up toggle to pull DTP1, DTP3, and MCV1 data respectively when compiling for the appropriate antigen
if (params$data == "dtp1") {
  file_path <- here::here("data", "wuenic2023_dtp1.csv")
} else if (params$data == "dtp3") {
  file_path <- here::here("data", "wuenic2023_dtp3.csv")
} else if (params$data == "mcv1") {
  file_path <- here::here("data", "wuenic2023_mcv1.csv") 
} else {
  msg <- sprintf("Unknown dataset requested: %s", params$data)
  stop(msg)
}

# Import WUENIC data
data_raw <- file_path %>%
  rio::import(header = TRUE) %>%
  tibble()
data_raw

# Import World Bank classification data
file_path_wb <- here::here("data", "wb_ig_2023.csv")
income <- file_path_wb %>%
  rio::import(header = TRUE, skip = 5) %>%
  tibble()
income <- income[-c(1:5), ]
income

# Import UN WPP data - two files are needed, one for historic values (up to 2021), and one for projections (2022)
file_path_unwpp_raw <- here::here("data", "WPP2024_GEN_F01_DEMOGRAPHIC_INDICATORS_COMPACT.csv")

unwpp_raw <- file_path_unwpp_raw %>%
  rio::import(header = TRUE, skip = 17) %>%
  tibble()
unwpp_raw
```

## Set-up output folders
Outputs are stored in two separate folders (in addition to the compiled report,
handled by the *reportfactory*), *figures/* and *csv/*. We make
sure these exist.

```{r}
# Make sure output folder exists
fig_folder <- here::here("figures_cov", params$data)
if (!dir.exists(fig_folder)) {
  dir.create(fig_folder, recursive = TRUE)
}

# Make sure output folder exists
csv_folder <- here::here("csv_cov", params$data)
if (!dir.exists(csv_folder)) {
  dir.create(csv_folder, recursive = TRUE)
}

```


# DESCRIPTIVE ANALYSES DATA PREPARATION
Here we prepare the dataset analysed called `x`.

## Data cleaning

### Coverage data
Steps taken for coverage data:

- rename variables 
- reshape data to long vs. wide format
- add categorisation columns (income group, region)
- select relevant columns only 

```{r}
# Clean and reshape coverage data
data_clean <- data_raw %>% 
  rename(iso_code = iso3) %>%
  mutate(region = countrycode(iso_code,
                              origin = "iso3c",
                              destination = "un.region.name")) %>%
  select(region, unicef_region, everything())

data_long <- data_clean %>%
  pivot_longer(-(region:vaccine), names_to = "year", values_to = "coverage")

names(data_long) <- tolower(names(data_long))
    
data_long %<>% 
  mutate(coverage = coverage / 100,
         year = as.integer(year))
data_long
```

### Income classification
Steps taken for income data:

- Extract income group as of 'Financial Year 2024' which refers to 2023
  classification
- Note that we used a fixed income group, i.e., not factoring in changes in
  2020-2022.

```{r}
# Extract relvant data from income dataset
income %<>% 
  select(V1, `2023`) %>%
  rename(iso_code = V1, `income_group` = `2023`) %>%
  mutate(
    income_group = recode_factor(income_group,
                               "L" = "LIC",
                               "LM" = "LMIC",
                               "UM" = "UMIC",
                               "H" = "HIC"))

# Join 'income' classification to main dataset
data_long %<>%
  inner_join(income, by = "iso_code")
data_long

```

### Population data
Steps taken for population data:
- Select relevant columns country and surviving infant estimate for each year,
  from 2000 to 2023
- Convert surviving infants estimates to numbers, and multiply them by 1000,
   since recorded in thousands in original dataset

```{r}
# Clean population data
unwpp_all <- unwpp_raw %>% 
  filter(Type == "Country/Area") %>%
  select(country = "Region, subregion, country or area *",
         iso_code = "ISO3 Alpha-code",
         population = "Total Population, as of 1 July (thousands)",
         surviving_infants = "Live Births Surviving to Age 1 (thousands)",
         year = "Year") %>%
  mutate(year = as.numeric(year)) %>%
  filter(year >= 2000,
         year <= 2023) %>%
  mutate(surviving_infants = as.numeric(gsub(",", "", surviving_infants)) * 1000,
         population = as.numeric(gsub(" ", "", population)) * 1000) %>%
  select(-country)

## Create wide dataset for total population only
unwpp_wide_all_pop <- unwpp_all %>%
  select(-surviving_infants) %>%
  pivot_wider(names_from = "year", names_prefix = "total_pop_", values_from = "population")
unwpp_wide_all_pop

```

### Consolidate and clean combined dataset
- Add in surviving infants data to the long dataset with coverage
- Calculate the number of reached (vaxxed) and unimmunised (missed) children per year based on coverage * target population size
```{r}
data_long %<>%
  left_join(unwpp_all, by = c("iso_code", "year")) %>%
  mutate(vaxxed = round(surviving_infants * coverage,0),
         missed = round(surviving_infants * (1-coverage),0)) 
```


Then we:
- reorder the variables we are interested in
- remove years prior to 2000
- remove entries where coverage is NA
- remove countries without data for the last 23 consecutive years (from 2000 to
  2023, inclusive)
```{r}
# Reorder dataset
x <- data_long %>%
  select(region, country, iso_code, income_group, year, coverage, vaxxed) %>%
  arrange(country, year)

# Filter for data from 2000 onwards (inclusive), remove NAs
x <- x %>%
  filter(year >= 2000, !is.na(coverage))

# Filter for countries with complete data from 2000 to 2023, i.e. 24 data points
complete_countries <- x %>%
  count(iso_code) %>%
  filter(n == 24) %>%
  pull(iso_code)

x %<>%
  filter(iso_code %in% complete_countries)

```

Finally we reshape as timeseries since `auto.arima` requires time series (`ts`) objects, which is essentially a wide
format for coverage data (rows = years, columns = countries).
```{r}
ts_df <- x %>%
  arrange(iso_code) %>%
  filter(year < 2020) %>% 
  select(year, iso_code, coverage) %>%
  pivot_wider(values_from = coverage, names_from = iso_code) %>%
  arrange(year) %>%
  select(-year) %>% 
  ts(start = 2000, freq = 1)

```

# ARIMA MODELLING
Here we apply ARIMA to each country using the years 2000-2019 as training set,
and derive a forecast for 2020 and 2021 with associated confidence intervals. The final
object will be called `res`, and will include these forecasts, the actual values
reported, and the corresponding coverage *deltas*, defined as (% reported - %
expected).

## Model parameters
Model "expected' 2020, 2021, 2022, and 2023 coverage based, in the absence of COVID, based on
2000-2019 trends.  ARIMA model uses timeseries data by country to (a) select the
most appropriate model, defined by three parameters (p, d, q) selected as
follows:

- p = number of autoregressive terms - based on minimisation of the AIC
- d = number of non-seasonal differences needed for stationarity - based on conducting KPSS tests
- q = number of lagged forecast errors in the prediction equation - based on minimisation of the AIC

and (b) predict 2020-2023 coverage based on this selected model.

## Fitting models: Forecast 2020-2023 coverage based on 2000-2019 trends
Note that in each WUENIC release, there may be 'retrospective' updates to coverage estimated and published in prior year datasets. For this reason we re-do analysis rather than leverage outputs from previously published papers (Evans & Jombart, 2022; and Evans et al., 2023).

```{r} 
#Forecast for 2020, 2021 and 2022 based on 2000-2019 inclusive
ts_forecasts <- lapply(ts_df, # iterate over all countries
                      function(y)
                        forecast(auto.arima(y, seasonal = FALSE),
                                 h = 4,     # forecast 2020-2023 inclusive
                                 level = 95 # 95% CIs
                                 ))

# extract forecasts and format output
forecasts <- lapply(ts_forecasts,
               function(e) c(method = e$method, as.data.frame(e))) %>%
  lapply(function(e) tibble(data.frame(e),year = c(2020,2021,2022,2023))) %>% 
  bind_rows(.id = "iso_code") %>%
  mutate(mean = `Point.Forecast`,
         lower_ci = `Lo.95`,
         upper_ci = `Hi.95`) %>%
  tibble() %>%
  select(iso_code, method, year, mean, lower_ci, upper_ci) %>%
  mutate(mean = if_else(mean > 0.99, 0.99, mean),          # Cap at 99% coverage as per WUENIC rules
         lower_ci = if_else(lower_ci < 0, 0, lower_ci),
         upper_ci = if_else(upper_ci > 0.99, 0.99, upper_ci),)
forecasts

```

## Create output 'forecasted' dataset
Here we:
- combine the reported WUENIC data for 2020-2023 with the ARIMA forecast coverage for 2020-2022
- calculate a variable called 'delta' which is the difference between reported and expected coverage (and the associated confidence intervals)

'res_all' is the dataset for all countries for 2020-2023 actual and estimated coverage
```{r}
x_temp <- x %>%
  filter(year == 2020 | year == 2021 | year == 2022 | year == 2023) %>%
  arrange(iso_code) 

res_all <- forecasts %>%
  left_join(x_temp, by = c("iso_code", "year")) %>%
  mutate(delta = coverage - mean,
         lower_delta = coverage - lower_ci,
         upper_delta = coverage - upper_ci,
         within_ci = (coverage >= lower_ci) & (coverage <= upper_ci),
         ci_width = upper_ci - lower_ci) %>%
  left_join(unwpp_all, by = c("iso_code", "year")) %>%
  mutate(expected_num = round(mean*surviving_infants,0),
           delta_num = vaxxed-expected_num)
```

## Create time series dataset that includes 2019 actuals and 2020-2022 expected and actuals for summary analyses
```{r}
# Extract forecasted coverage from results dataset
temp_forecast_data <- res_all %>%
  select(iso_code, year, forecast_coverage = mean)

# Create temporary dataset with the 2019 actuals coverage
temp_cov_2019 <- x %>%
  filter(iso_code %in% temp_forecast_data$iso_code,
         year == 2019) %>%
  select(iso_code, coverage_2019 = coverage)

# Create combined dataset including actuals and forecasted data; and include population data to translate coverage to number of immunisations
timeseries_dat <- x %>%
  filter(iso_code %in% temp_forecast_data$iso_code) %>% 
  left_join(temp_forecast_data, by = c("year", "iso_code")) %>%
  left_join(temp_cov_2019, by = "iso_code") %>%
  mutate(gap_2019_to_forecast = ifelse(!is.na(forecast_coverage), 
                                       forecast_coverage - coverage_2019, "")) %>%
  select(-c(coverage_2019)) %>%
  left_join(unwpp_all, by = c("iso_code","year")) %>%
  mutate(reached_kids = round(coverage * surviving_infants,0),
         expected_kids = case_when(is.na(forecast_coverage) ~ round(reached_kids,0),
                                   .default = round(forecast_coverage * surviving_infants,0)),
         extra_missed = expected_kids - reached_kids)

```

## Set-up output figure set-ups for analyses
- Set-up shortcuts for colour schemes for income group and regional analyses
```{r}
# Colour scheme for regions
n_regions <- res_all %>%
  pull(region) %>%
  unique() %>%
  length()
region_pal <- wes_palette("Darjeeling1", n_regions, type = "discrete")

# Colour scheme for income groups
n_income <- res_all %>%
  filter(income_group != "") %>%
  pull(income_group) %>%
  unique() %>%
  length()
income_pal <- wes_palette("BottleRocket2", n_income, type = "discrete")

income_pal2 <- wes_palette("FantasticFox1", 5, type = "discrete")
```

## Visualise coverage timeseries for subset of countries
- This is included in the code for explanatory purposes. The Figure is not included in the journal paper.
```{r, out.width = "100%", fig.width = 10, fig.height = 3}
# Indentify largest countries in dataset
unwpp_ordered <- unwpp_all %>%
  filter(year == 2023) %>%
  arrange(-surviving_infants) %>%
  filter(iso_code %in% res_all$iso_code) %>% #filter to only those countries in final dataset
  select(iso_code, surviving_infants) %>%
  head(5)
unwpp_ordered

large_pops <- unwpp_ordered$iso_code

# Identify improved countries in dataset
improved <- res_all %>%
  filter(within_ci == FALSE & coverage > mean)

improved_countries <- improved$iso_code %>% unique()

# Filter countries - can change to be 'improved_countries' or 'large_pops'
x_plot <- x %>%
  filter(iso_code %in% improved_countries)

res_plot <- res_all %>%
  filter(iso_code %in% improved_countries)

# Generate plot
arima_color <- "#ac3973"

fig_forecasts <- ggplot(data = x_plot, aes(x = year, y = coverage)) +
  theme_bw() +
  geom_point(alpha = 0.8) +
  geom_line(alpha = 0.3) +
  geom_errorbar(data = res_plot, aes(x = year, ymin = lower_ci, ymax = upper_ci),
                color = arima_color) +
  geom_point(data = res_plot, aes(y = mean), shape = 3, color = arima_color) +
  facet_wrap(~ country, nrow = 1, scales = "free_y",
             labeller = label_wrap_gen(25)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1L), 
                     limits = c(NA,1),
                     n.breaks = 5) +
  labs(y = "Coverage (%)",
       x = "Year") +
  theme(axis.text.x = element_text(angle=45,hjust = 1, size = 11),
        axis.text.y = element_text(size = 11), 
        strip.text.x = element_text(size = 11), 
        axis.title = element_text(size = 14))
fig_forecasts

# Save file 
ggsave(filename = here::here(fig_folder, paste("Supplementary_materials_Examples_",params$data,".png", sep = "")),
       plot = fig_forecasts, 
       width = 30, height = 10, units = "cm")

```

# DESCRIBE GLOBAL TRENDS

## T-tests 
### Coverage
- We conduct a t-test for each year (2020-2023) comparing expected to reported coverage
- The results from these t-tests are reported for DTP3 in the paper Table 2; and for DTP1 in Supplementary Materials Table S1.
```{r}
# Conduct t-test across each year
ttests <- res_all %>%
  split(res_all$year) %>%
  lapply(function(e) t.test(x = e$delta,  
                                alternative = "two.sided"))

# Extract t-test results per year
ttests <- lapply(names(ttests), function(year) {
  test <- ttests[[year]]
  data.frame(
    year = year,
    delta = test$estimate,
    t.value = test$statistic,
    p.value = test$p.value,
    conf.low = test$conf.int[1],
    conf.high = test$conf.int[2],
    sample_size = test$parameter+1
  )
})

ttests_tab <- do.call(rbind, ttests) %>%
  mutate(year = as.numeric(year))

# Include the reported and expected coverage
cov_summary <- res_all %>%
  split(res_all$year) %>%
  lapply(function(e) {
    data.frame(
      year = unique(e$year),
      reported = mean(e$coverage, na.rm = TRUE),
      expected = mean(e$mean, na.rm = TRUE)
    )
  })

cov_tab <- do.call(rbind, cov_summary)

# Combine results
ttests_summary <- ttests_tab %>%
  left_join(cov_tab, by = "year") %>%
  mutate(lower95 = reported - abs(conf.low),
         upper95 = reported - abs(conf.high)) %>%
  select(year, expected, reported, delta, lower95, upper95, 
         conf.high, conf.low, p.value, sample_size)

# Format output for paper
tt_total_paper <- ttests_summary %>%
  mutate(reported = round(reported, digits = 3) * 100,
         Reported = paste(reported,"%", sep = ""),
         expected = round(expected, digits = 3) * 100,
         Expected = paste(expected,"%", sep = ""),
         delta = round(delta, digits = 3) * 100,
         lower_ci = round(conf.high, digits = 3) * 100,
         upper_ci = round(conf.low, digits = 3) * 100,
         Delta = paste(delta,"% [",lower_ci,"%; ",upper_ci,"%]",sep = ""),
         `p-value` = ifelse(p.value < 0.0001, "< 0.0001", 
                            signif(p.value, digits = 2))) %>%
  select(Year = year, Expected, Reported, Delta, `p-value`)
tt_total_paper

write.csv(
  tt_total_paper,
  here::here(csv_folder, paste("table_paper_ttest_cov_unweighted_",params$data,".csv", sep = "")),
  row.names = FALSE)

# Create prettier Flextable for outputs
tt_total_tab <- flextable(tt_total_paper) %>%
  theme_vanilla() %>%
  autofit()
tt_total_tab

doc <- read_docx()
doc <- body_add_flextable(doc, value = tt_total_tab)
print(doc, target = paste("../figures_cov/",params$data,"/tt_unweighted_tab_",
                          params$data,".docx", sep =""))

```

### Number of immunisations
- We conduct a t-test for each year (2020-2023) comparing expected to reported number of immunisations
- The results from these t-tests are reported for DTP3 in the paper Table 3; and for DTP1 in Supplementary Materials Table S2.
```{r}
# Conduct t-test across each year
ttests_num <- res_all %>%
  split(res_all$year) %>%
  lapply(function(e) t.test(x = e$delta_num,  
                                alternative = "two.sided"))

# Extract t-test results per year
ttests_num <- lapply(names(ttests_num), function(year) {
  test <- ttests_num[[year]]
  data.frame(
    year = year,
    delta = test$estimate,
    t.value = test$statistic,
    p.value = test$p.value,
    conf.low = test$conf.int[1],
    conf.high = test$conf.int[2],
    sample_size = test$parameter+1
  )
})

ttests_num_tab <- do.call(rbind, ttests_num) %>%
  mutate(year = as.numeric(year))

# Include the reported and expected average number of immunisations
num_summary <- res_all %>%
  split(res_all$year) %>%
  lapply(function(e) {
    data.frame(
      year = unique(e$year),
      reported = mean(e$vaxxed, na.rm = TRUE),
      expected = mean(e$expected_num, na.rm = TRUE)
    )
  })

num_tab <- do.call(rbind, num_summary)

# Combine results
ttests_num_summary <- ttests_num_tab %>%
  left_join(num_tab, by = "year") %>%
  mutate(lower95 = reported - abs(conf.low),
         upper95 = reported - abs(conf.high)) %>%
  select(year, expected, reported, delta, lower95, upper95, 
         conf.high, conf.low, p.value, sample_size)

# Format output for paper
tt_num_paper <- ttests_num_summary %>%
  mutate(
    Year = year,
    Reported = comma(reported, digits = 0),
    Expected = comma(expected, digits = 0),
    delta = comma(delta, digits = 0),
    lower_ci = comma(conf.high, digits = 0),
    upper_ci = comma(conf.low, digits = 0),
    Delta = paste(delta, " [", 
                  lower_ci, " ; ", 
                  upper_ci, "]", 
                  sep = ""),
    `p-value` = ifelse(p.value < 0.0001, "< 0.0001", signif(p.value, digits = 2))) %>%
    select(Year, Expected, Reported, Delta, `p-value`)
tt_num_paper

write.csv(
  tt_total_paper,
  here::here(csv_folder, paste("table_paper_ttest_cov_num_",params$data,".csv", sep = "")),
  row.names = FALSE)

# Create prettier Flextable for outputs
tt_num_tab <- flextable(tt_num_paper) %>%
  theme_vanilla() %>%
  autofit()
tt_num_tab

doc <- read_docx()
doc <- body_add_flextable(doc, value = tt_num_tab)
print(doc, target = paste("../figures_cov/",params$data,"/tt_num_tab_",
                          params$data,".docx", sep =""))

```

## Visualise coverage trends globally

### Coverage
- We produce a figure to show the trend in global vaccine coverage (not weighted for population size)
```{r}
# Consolidate data
unweighted <- timeseries_dat %>%
  group_by(year) %>%
  summarise(achieved = mean(coverage, na.rm = TRUE),
            expected = mean(forecast_coverage, na.rm = TRUE)) %>%
  mutate(expected = case_when(expected == "NaN" ~ achieved,
                              .default = expected),
         target = 0.99) %>%
  pivot_longer(!year,
               names_to = "category",
               values_to = "percent") %>%
  mutate(remove = case_when(year <= 2018 & category == "expected" ~ 1,
                            .default = 0)) %>%
  filter(remove != 1) %>%
  select(-remove)

# Plot graph showing global weighted coverage per year from 2000-2023
global_cov_unweighted <- ggplot(data = unweighted, aes(x = year, y = percent)) +
  geom_point(aes(colour = category), size = 2) +
  geom_line(aes(colour = category), linewidth = 1, alpha = 0.5) +
  geom_hline(yintercept = 0.99, linewidth = 1, alpha = 0.8, 
             linetype = "dashed", colour = "red")+
  scale_y_continuous(labels = scales::percent,
                     n.breaks = 10, 
                     limits = c(0.8,1)) +
  scale_color_manual("", values = c(achieved = "#5884C3", 
                                    expected = "darkblue", 
                                    target = "red"), 
                     labels = 
                       c(paste("Reported",toupper(params$data)),
                         paste("Modelled",toupper(params$data)),
                         "Target coverage: 99%")) +
  labs(y = "Mean coverage (%)",
       x = "Year") +
  theme_light() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),
    axis.text.y = element_text(size = 10),
    strip.text.x = element_text(size = 10),
    axis.title = element_text(size = 11, face = "bold"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    plot.title = element_text(size = 11, face = "bold", hjust = 0.5),
    legend.position = c(0.7, 0.2),
    legend.background = element_rect(colour = NA, fill = NA))
global_cov_unweighted
```

### Number of immunisations
- We produce a figure to show the trend in number of immunisations globally over time
```{r}
# Weighted dataset
weighted <- timeseries_dat %>%
  group_by(year) %>%
  summarise(reached = sum(reached_kids),
            expected = sum(expected_kids),
            si = sum(surviving_infants)) %>%
  mutate(reached_perc = reached/si,
         expected_perc = expected/si)

# Restructure
weighted_long <- weighted %>%
  mutate(si_target = round(0.99*si)) %>%
  select(year, reached, expected, si_target) %>%
  pivot_longer(!year, names_to = "category", values_to = "number") %>%
  mutate(remove = case_when(year <= 2018 & category == "expected" ~ 1,
                            .default = 0)) %>%
  filter(remove != 1) %>%
  select(-remove)
  
global_num <- ggplot(data = weighted_long, aes(x = year, y = number)) +
  geom_point(aes(colour = category), size = 2) +
  geom_line(aes(colour = category), linewidth = 1, alpha = 0.5) +
  scale_y_continuous(labels = scales::unit_format(unit = "M", scale = 1e-6),
                     n.breaks = 10,
                     limits = c(90000000,145000000)) +
  scale_color_manual("", values = c(reached = "#5884C3", 
                                    expected = "darkblue", 
                                    si_target = "red"), 
                     labels = 
                       c(paste("Modelled",toupper(params$data)),
                         paste("Reported",toupper(params$data)),
                         "Target population: \n99% surviving infants")) +
  labs(y = "Number of immunised children",
       x = "Year") +
  theme_light() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),
    axis.text.y = element_text(size = 10),
    strip.text.x = element_text(size = 10),
    axis.title = element_text(size = 11, face = "bold"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    plot.title = element_text(size = 11, face = "bold", hjust = 0.5),
    legend.position = c(0.7, 0.2),
    legend.background = element_rect(colour = NA, fill = NA))
global_num
```
### Create combined figure
- Note that for the paper we run this for DTP1 and DTP3 separately and combine to give a 4-panel figure showing mean coverage and number of immunisations over time for DTP1 and DTP3. This is shown as Figure 1 in the paper.
```{r}
global <- plot_grid(global_cov_unweighted, global_num,
                    ncol = 2,
                    labels = c("A", "B"))

global

# Save file 
ggsave(filename = here::here(fig_folder, 
                             paste("Paper_global_immunisation_trends_",params$data,".png", 
                                   sep = "")),
       plot = global, 
       width = 30, height = 10, units = "cm")

```


# DESCRIBE COUNTRY-LEVEL TRENDS

## Classify country recovery status
```{r}
# Count number of countries where reported coverage is outside confidence intervals of expected coverage
conf <- res_all %>% 
  filter(!within_ci) %>%
  mutate(classification = ifelse(coverage < lower_ci, "lower", "higher")) %>%
  group_by(year, classification) %>%
  count() %>%
  pivot_wider(names_from = classification, values_from = n)
conf

# Classify country recovery status in 2023 
explanatory_data <- res_all %>%
  select(country, iso_code, year, expected = mean, lower_ci, upper_ci, reported = coverage, delta, lower_delta, upper_delta, within_ci, population, surviving_infants, expected_num, vaxxed, delta_num) %>%
  mutate(class = case_when(within_ci == FALSE & reported <= expected ~ "decline",
                           within_ci == TRUE ~ "within_ci",
                           .default = "improve"),
         classification = case_when(within_ci == FALSE & reported <= expected ~ 0,
                                    within_ci == FALSE & reported >= expected ~ 2,
                                    .default = 1))

class_data <- explanatory_data %>% 
  filter(year == 2023) %>%
  select(iso_code, class) %>%
  mutate(class = factor(class, levels = c("decline", "within_ci", "improve")))
```

## Describe missed children for countries outside of confidence intervals
```{r}
# Quantify expected vs. reported missed immunisations for countries outside confidence intervals
res_ci <- res_all %>%
  filter(within_ci == FALSE) %>%
  group_by(year) %>%
  summarise(expected = sum(expected_num),
            reached = sum(vaxxed),
            delta = abs(sum(delta_num)))
res_ci

# Create prettier Flextable for outputs
res_ci_tab <- flextable(res_ci) %>%
  theme_vanilla() %>%
  autofit()

doc <- read_docx()
doc <- body_add_flextable(doc, value = res_ci_tab)
print(doc, target = paste("../figures_cov/",params$data,"/res_ci_tab_",
                          params$data,".docx", sep =""))

```

## Map country classification
- We create a global map, colour-coded by recovery status. This is included as Figure 2 in the paper.
- The labels detail the numbers and percentages for DTP3 country classification
```{r}
# Plot coverage deltas visually
  # Download country map data
  countries <- ne_countries(returnclass = "sf")

  # Create dataset
    map_data <- countries %>%
      left_join(class_data, by = c("iso_a3" = "iso_code"))

  # Palette for map
    improved <- "darkcyan"
    worse <- "magenta4"
    within_ci <- "goldenrod3"
 
     # Plot deltas
    recovery_map <- ggplot(data = map_data) +
      geom_sf(aes(fill = class), color = "black", size = 0.2) + 
      scale_fill_manual(values = c(worse, within_ci, improved),
                        na.value = "grey80",
                        labels = c("Decline (n = 32/190, 16.8%)",
                                   "Within confidence intervals \n(n = 155/190, 81.6%)",
                                   "Improved (n = 3/190, 1.6%)"))+ 
      labs(fill = "Country classification") +
      theme_minimal() +
      theme(panel.background = element_rect(fill = "white"),
            legend.position = c(0.15, 0.35),
            legend.text = element_text(size = 9),
            legend.title = element_text(size = 9, face = "bold"))

    recovery_map
    
# Save figure
ggsave(filename = here::here(fig_folder, paste("Paper_Geographic_recovery_",params$data,".png", sep = "")),
       plot = recovery_map, 
       width = 30, height = 15, units = "cm")
```
## Produce table of coverage declines and number of immunisations for countries below expectations
- This is reported in Supplementary Materials Table S3.
```{r}
# Filter for relevant information
below_ci <- res_all %>%
  filter(year == 2023 & within_ci == FALSE & coverage < mean) %>%
  select(country, expected_cov = mean, low_ci = lower_ci, upp_ci = upper_ci, reported_cov = coverage, delta, delta_num) %>%
  mutate(expected_cov = round(expected_cov*100,1),
         low_ci = round(low_ci*100,1),
         upp_ci = round(upp_ci*100,1),
         reported_cov = formatC(round(reported_cov*100,1),1,format = "f"),
         delta = round(delta*100,1)) %>%
  arrange(delta_num) %>%
  rename(Country = country,
         `Expected coverage (%)` = expected_cov,
         `95% Confidence Interval (low, %)` = low_ci,
         `95% Confidence Interval (high, %)` = upp_ci,
         `Reported coverage (%)` = reported_cov,
         `Delta (%)` = delta,
         `Delta (number immunisations)` = delta_num) 

# Save as CSV
write.csv(
  below_ci,
  here::here(csv_folder, paste("Supplementary_table_1_below_ci_",params$data,".csv", sep = "")),
  row.names = FALSE)

# Create prettier Flextable for outputs
below_ci_tab <- flextable(below_ci) %>%
  theme_vanilla() %>%
  autofit()

doc <- read_docx()
doc <- body_add_flextable(doc, value = below_ci_tab)
print(doc, target = paste("../csv_cov/",params$data,"/below_ci_tab_",
                          params$data,".docx", sep =""))
```

#EXPLANATORY ANALYSES

## Import additional datasets
We import datasets describing:
- Pre-pandemic immunisation coverage for DTP3, as tracker of EPI performance
- Number of doctors and nurses per capita, as indicator of health workforce capacity
- Universal health coverage index, as indicator of health system strength
- Excess mortality estimates from during COVID-19, as indicator of COVID-19 health disruption
- Health financing data (per capita, and PPP), as indicator of financial investment in health systems
- Gross Domestic Product data (per capita and PPP), as indicator of total country financial resources
- COVID-19 vaccine mandate/policy stringency data, to explore if COVID-19 vaccine mandates may have impacted broader vaccine hesitancy
- Global Health Security index score data, as indicator of broad health system preparedness for managing epidemics and pandemic responses
- Pandemic policy data, spanning containment, economic, and health system policy response stringency during COVID-19 pandemic, as indicator of country responses to pandemic

We also use the population size data from UNWPP (as imported above) to investigate if country size is related to residual coverage disruption
```{r}
# Import Pre-pandemic immunisation system strength
file_path_RI <- here::here("data", "wuenic2023_dtp3.csv")

ri_data <- file_path_RI %>%
  rio::import(header = TRUE) %>%
  tibble()
ri_data

# Import RI schedules
file_path_schedules <- here::here("data", "vaccine-schedule-data.csv")

schedule_data <- file_path_schedules %>%
  rio::import(header = TRUE) %>%
  tibble()
schedule_data

# Import Health workforce data
file_path_doctors <- here::here("data", "doctors.csv")
file_path_nurses <- here::here("data", "nurses.csv")

doctors_data <- file_path_doctors %>%
  rio::import(header = TRUE) %>%
  tibble()
doctors_data

nurses_data <- file_path_nurses %>%
  rio::import(header = TRUE) %>%
  tibble()
nurses_data

# Import Universal Health Coverage data
file_path_uhc <- here::here("data", "uhc_data.csv")

uhc_data <- file_path_uhc %>%
  rio::import(header = TRUE) %>%
  tibble()
uhc_data

# Import excess mortality data
file_path_mortality_econ2 <- here::here("data", "excess-deaths-cumulative-per-100k-economist.csv")

mortality_data_econ2 <- file_path_mortality_econ2 %>%
  rio::import(header = TRUE) %>%
  tibble()
mortality_data_econ2

# Import health financing dataset
file_path_hf_ppp <- here::here("data", "hf_ppp_2015_2022.csv")

hf_ppp_data <- file_path_hf_ppp %>%
  rio::import(header = TRUE) %>%
  tibble()
hf_ppp_data

# Import GDP data
file_path_gdp <- here::here("data", "World_Bank_GDP_per_capita_PPP.csv")

gdp_data <- file_path_gdp %>%
  rio::import(header = TRUE, skip = 4) %>%
  tibble()
gdp_data

# Import vaccine policy data
file_path_vx_policy <- here::here("data", "OxCGRT_vaccines_full_national_v1.csv")

vx_policy_data <- file_path_vx_policy  %>%
  rio::import(header = TRUE) %>%
  tibble()
vx_policy_data

# Import Global Health Security preparedness data
file_path_ghs <- here::here("data", "2021-GHS-Index-April-2022.csv")

ghs_data <- file_path_ghs %>%
  rio::import(header = TRUE) %>%
  tibble()
ghs_data

# Import pandemic policy data
file_path_policy <- here::here("data", "OxCGRT_compact_national_v1.csv")

policy_data <- file_path_policy %>%
  rio::import(header = TRUE) %>%
  tibble()
policy_data

```

## Set-up exploratory dataset
- Initiate tracker to keep all cleaned data per country together
```{r}
# Initiate tracker
tracker <- explanatory_data %>%
  select(iso_code) %>%
  unique()
```

### Pre-pandemic immunisation system strength
- Source: WUENIC coverage data, updated July 2024
```{r}
# Clean
ri <- ri_data %>%
  select(iso_code = iso3,
         `2015`, `2016`, `2017`, `2018`, `2019`) %>%
  rowwise() %>%
  mutate(average_cov = mean(c_across(where(is.numeric)), na.rm = TRUE)) %>%
  select(iso_code, average_cov)

# Add data to tracker
tracker %<>%
  left_join(ri, by = "iso_code")
```

### RI schedules
- Source: WHO
```{r}
# Clean
schedules <- schedule_data %>%
  select(iso_code = ISO_3_CODE, vx = VACCINECODE, vx_desc = VACCINE_DESCRIPTION, target = TARGETPOP_DESCRIPTION, year = YEAR, geog = GEOAREA, age = AGEADMINISTERED, dose_num = SCHEDULEROUNDS)

# Clean data to extract vaccines introduced in infant RI schedule per country (based on latest nationwide schedule for infants)
ri_schedules <- schedules %>%
  filter(geog == "NATIONAL",
         year == 2023,       
         target == "General/routine", 
         dose_num == 1,      # Filter for first dose of course, to minimise number of duplications per vaccine type
         !grepl("Y", age),   # Filter out schedules delivered to older children or adults. Y stands for Y and vaccines are coded in terms of B for birth, D for day number, M for month number and Y for year
         !grepl("adult", vx_desc)) %>% # Filter for infant schedules i.e., not adult/special populations
  unique() %>%
  select(-c(geog, year, target))

# Summarise number of vaccines in RI schedule per country
count_schedule <- ri_schedules %>%
  group_by(iso_code) %>%
  count() %>%
  select(iso_code, ri_intros = n)

# Add data to tracker
tracker %<>%
  left_join(count_schedule, by = "iso_code")
```

### Health workforce
- Source: WHO 
```{r}
# Filter and select data
doctors <- doctors_data %>%
  select(iso_code = SpatialDimValueCode, year = Period, value = Value) %>%
  filter(year >= 2015 & year <= 2019) %>%
  pivot_wider(names_from = year, values_from = value) %>%
  rowwise() %>%
  mutate(mean_doctors = mean(c_across(where(is.numeric)), na.rm = TRUE)) %>%
  select(iso_code, mean_doctors) %>%
  as_tibble() 

nurses <- nurses_data %>%
  select(iso_code = SpatialDimValueCode, year = Period, value = Value) %>%
  filter(year >= 2015 & year <= 2019) %>%
  pivot_wider(names_from = year, values_from = value) %>%
  rowwise() %>%
  mutate(mean_nurses = mean(c_across(where(is.numeric)), na.rm = TRUE)) %>%
  select(iso_code, mean_nurses) %>%
  as_tibble() 

# Add to tracker
tracker %<>%
  left_join(doctors, by = "iso_code") %>%
  left_join(nurses, by = "iso_code")

```

### Universal health coverage data
- Source: WHO UHC indicator 3.8.1
```{r}
# Clean data
uhc <- uhc_data %>%
  filter(Period >= 2015 & Period <= 2022) %>%
  select(iso_code = SpatialDimValueCode, uhc_index = FactValueNumeric, year = Period) %>%
  pivot_wider(values_from = uhc_index, names_from = year, names_prefix = "uhc_index_") %>%
  mutate(uhc_mean_index = round(rowMeans(select(.,uhc_index_2015, uhc_index_2017, uhc_index_2019), na.rm = TRUE), digits = 1)) %>%
  select(iso_code, uhc_mean_index)

# Add to tracker
tracker %<>%
  left_join(uhc, by = "iso_code")
```

### Global health security data
- Source: Global Health Security Index
```{r}
# Clean data
ghs <- ghs_data %>%
  filter(Year == 2019) %>%
        mutate(iso_code = countrycode(Country,
                                      origin = "country.name",
                                      destination = "iso3c")) %>%
        select(c(iso_code, ghs_index = `OVERALL SCORE`)) 

# Add to tracker
tracker %<>%
  left_join(ghs, by = "iso_code")
```

### Excess mortality data
- Source: The Economist
```{r}
# clean data
mort_dat_long <- mortality_data_econ2 %>%
  mutate(Day = as.character(Day),
         year = as.numeric(substr(Day, 1, 4))) %>%
  filter(Day == "2020-12-28" | Day == "2021-12-27" | Day == "2022-12-26") %>% #Select year end data for each year only
  select(iso_code = Code, year,
         econ_mort_excess = `Cumulative excess deaths per 100,000 people (central estimate)`)

# Add data to tracker
tracker %<>%
  left_join(mort_dat_long, by = "iso_code")


```

### Health financing
- Source: WHO Global Health Expenditure Database (GHED)
```{r}
# Clean data
hf_ppp <- hf_ppp_data %>%
  mutate(iso_code = countrycode(Countries,
                                origin = "country.name",
                                destination = "iso3c")) %>%
  select(iso_code, indicator = Indicators,  `2015`, `2016`, `2017`, `2018`, `2019`) %>% 
  mutate(`2015` = as.numeric(gsub(",", "",`2015`)),
         `2016` = as.numeric(gsub(",", "",`2016`)),
         `2017` = as.numeric(gsub(",", "",`2017`)),
         `2018` = as.numeric(gsub(",", "",`2018`)),
         `2019` = as.numeric(gsub(",", "",`2019`))) %>%
      rowwise() %>%
      mutate(average = mean(c_across(where(is.numeric)), na.rm = TRUE)) %>%
      select(iso_code, indicator, average) %>%
      pivot_wider(names_from = indicator,
                  values_from = average) %>%
      rename(che = `Current Health Expenditure (CHE)`,
             gov = `Domestic General Government Health Expenditure (GGHE-D)`,
             ext = `External Health Expenditure (EXT)`,
             private = `Domestic Private Health Expenditure (PVT-D)`) %>%
      mutate(che = round(as.numeric(gsub(",", "",che)), digits = 1),
             gov = round(as.numeric(gsub(",", "",gov)), digits = 1),
             ext = round(as.numeric(gsub(",", "",ext)), digits = 1),
             private = round(as.numeric(gsub(",", "",private)), digits = 1),
             ext = replace_na(ext, replace = 0)) 

hf_summary <- hf_ppp %>%
  select(iso_code, gov, ext, private)

# Add to tracker
tracker %<>%
  left_join(hf_summary, by = "iso_code")
```

### Pandemic policies
- Source: Oxford COVID-19 Government Response Tracker
```{r}
# Calculate and extract summary annual indicators
policy <- policy_data %>%
  filter(Jurisdiction == "NAT_TOTAL") %>% #Check that policies apply at national level
  mutate(year = substr(Date, 1, 4),
         month = substr(Date, 5, 6),
         day = substr(Date, 7, 8),
         date = dmy(paste(day, month, year, sep = "-"))) %>%
  select(iso_code = CountryCode,
         year,
         c1m_school = `C1M_School closing`,
         c2m_work = `C2M_Workplace closing`,
         c3m_events = `C3M_Cancel public events`,
         c4m_gatherings = `C4M_Restrictions on gatherings`,
         c5m_public_transit = `C5M_Close public transport`,
         c6m_stay_at_home = `C6M_Stay at home requirements`,
         c7m_internal_mov = `C7M_Restrictions on internal movement`,
         c8ev_international_travel = `C8EV_International travel controls`,
         e1_income_support = `E1_Income support`,
         e2_debt_relief = `E2_Debt/contract relief`,
         e3_fiscal_measures = `E3_Fiscal measures`,
         e4_international_support = `E4_International support`,
         h1_public_info = `H1_Public information campaigns`,
         h2_testing_policy = `H2_Testing policy`,
         h3_contact_tracing = `H3_Contact tracing`,
         h4_healthcare_invest = `H4_Emergency investment in healthcare`,
         h5_vx_invest = `H5_Investment in vaccines`,
         h6m_masks = `H6M_Facial Coverings`,
         h7_vx_policy = `H7_Vaccination policy`,
         h8m_elderly = `H8M_Protection of elderly people`) %>%
  filter(year > 2019 & year < 2023) %>%
  group_by(iso_code, year) %>%
  summarise(across(everything(), mean)) %>% 
  select(-c(e3_fiscal_measures,
            e4_international_support,
            h4_healthcare_invest,
            h5_vx_invest)) %>% #Remove 4 variables since noted as incomplete/need caution
  mutate(year = as.numeric(year))
 
# Add data to tracker
tracker %<>%
  left_join(policy, by = c("iso_code", "year"))
```

### GDP
- Source: World Bank
```{r}
# Clean data
gdp <- gdp_data %>%
  select(iso_code = `Country Code`,
         `2015`, `2016`, `2017`, `2018`, `2019`) %>%
  rowwise() %>%
  mutate(average_gdp = mean(c_across(where(is.numeric)), na.rm = TRUE)) %>%
  select(iso_code, average_gdp)

# Add data to tracker
tracker %<>%
  left_join(gdp, by = "iso_code")
```

### Vaccine mandate data
- Source: Oxford COVID-19 Government Response Tracker
```{r}
# Clean data
vx_policy <- vx_policy_data %>%
  filter(Jurisdiction == "NAT_TOTAL") %>% #Check that policies apply at national level
  rename(iso_code = CountryCode) %>%
  mutate(year = substr(Date, 1, 4),
         month = substr(Date, 5, 6),
         day = substr(Date, 7, 8),
         date = dmy(paste(day, month, year, sep = "-"))) %>%
  group_by(iso_code, year) %>%
  select(iso_code, year, starts_with("V4")) %>%
  summarise(across(everything(), mean))

# Summarise into single indicator
vx_policy_highlevel <- vx_policy %>%
  filter(year == 2022) %>% # Take 2022 stringency estimate since this has most data and most recent
  select(iso_code, vx_stringency = `V4_Mandatory Vaccination (summary)`)

# Add data to tracker
tracker %<>%
  left_join(vx_policy_highlevel, by = "iso_code")

```

### Country size
- Source: UN WPP
```{r}
# Extract population data - and calculate mean coverage for 2020-2023 to factor in changes in population size within countries
pop <- unwpp_all %>%
  filter(year >= 2020,
         year <= 2023) %>%
  select(iso_code, population, year)

# Add to tracker
tracker %<>%
  left_join(pop, by = c("iso_code", "year"))
```

## Explore consolidated explanatory dataset
- We explore and describe the explanatory dataset variables, including looking at variable distribution; missing values; and between variable correlation

### Data completeness
```{r}
na_summary <- tracker %>%
  summarise(across(everything(), ~ sum(is.na(.)), .names = "NA_count_{.col}")) %>%
  pivot_longer(everything(), names_to = "Column", values_to = "NA_Count") %>%
  mutate(Column = str_remove(Column, "NA_count_"))

na_summary
```
- We are missing data for 75 countries for vaccine hesitancy. To keep the dataset larger for exploring relationships with other variables, we remove this variable for further analyses

### Create final dataset for explanatory analyses
```{r}
tracker2 <- tracker %>%
  select(-vx_stringency) %>%
  mutate(year = as.numeric(year))

tracker_2023 <- tracker2 %>%
  group_by(iso_code) %>%
  summarise(across(everything(), mean, na.rm = TRUE)) %>%
  mutate(year = 2023)

tracker_combo <- tracker2 %>%
  rbind(tracker_2023)

explore_data <- explanatory_data %>%
  select(iso_code, year, classification) %>% #Extract response variable
  mutate(classification = factor(classification, levels = c(0, 1, 2))) %>%
  left_join(tracker_combo, by = c("iso_code", "year")) %>% # Add predictors
  na.omit() #Remove incomplete line items
explore_data
```
### Variable distribution
```{r}
# Restructure data for easier plotting
  predictor_data <- explore_data %>%
    select(-c(iso_code, year, classification))
  
# Reshape the data into a long format for ggplot2
  long_predict_data <- predictor_data %>%
    pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# Create the facet plot of histograms
  ggplot(long_predict_data, aes(x = value)) +
    geom_histogram(bins = 30, fill = "blue", alpha = 0.7) + # Adjust bins if needed
    facet_wrap(~variable, scales = "free") +
    theme_minimal() +
    labs(title = "Histograms of Predictor Variables",
         x = "Value",
         y = "Frequency")

```

### Check data normally distributed
```{r}
# Define inputs/set-up areas to store results
  check_normality <- function(data, exclude_vars) {

  predictors <- setdiff(names(data), exclude_vars)
  
  results <- list()
  
  # Loop through each predictor
  for (predictor in predictors) {
    class_0 <- shapiro.test(data[[predictor]][data$classification == 0])
    class_1 <- shapiro.test(data[[predictor]][data$classification == 1])
    
    # Store the p-values
    results[[predictor]] <- list(
      class_0_p = round(class_0$p.value, digits = 3),
      class_1_p = round(class_1$p.value, digits = 3))
  }
  
  # Convert results to a data frame for easier reading
  results_df <- do.call(rbind, lapply(results, as.data.frame))
  rownames(results_df) <- predictors
  return(results_df)
}

# Run the function on your dataset
normality_results <- check_normality(
  data = explore_data, 
  exclude_vars = c("classification", "iso_code", "year")
)

# View the results
print(normality_results)
```
### Between variable correlation
- The resulting correlation matrix is reported in Supplementary Materials Figure S1.
```{r}
# Format and label predictor data for easy interpretation
predictors_labelled <- explore_data[-(1:3)] %>%
  rename(`Pre-pandemic DTP3 coverage` = average_cov,
         `Number of RI vaccines` = ri_intros,
         `Number of doctors` = mean_doctors,
         `Number of nurses` = mean_nurses,
         `Health system strength` = uhc_mean_index,
         `Global Health Security index` = ghs_index,
         `Excess mortality` = econ_mort_excess,
         `Government health financing` = gov,
         `Private health financing` = private,
         `External (donor) health financing` = ext,
         `School closures` = c1m_school,
         `Work closures` = c2m_work,
         `Event cancellations` = c3m_events,
         `Gathering restrictions` = c4m_gatherings,
         `Public transit closures` = c5m_public_transit,
         `Stay at home orders` = c6m_stay_at_home,
         `Internal movement restrictions` = c7m_internal_mov,
         `International travel restrictions` = c8ev_international_travel,
         `Income support` = e1_income_support,
         `Debt relief` = e2_debt_relief,
         `Public information` = h1_public_info,
         `COVID-19 testing policy` = h2_testing_policy,
         `COVID-19 contact tracing` = h3_contact_tracing,
         `Mask wearing requirements` = h6m_masks,
         `COVID-19 vaccine availability` = h7_vx_policy,
         `Elderly sheltering` = h8m_elderly,
         `Gross Domestic Product` = average_gdp,
         `Population size` = population) 

corr_data <- cor(predictors_labelled)

# Create correlation matrix
predictor_correlation <- ggcorrplot(corr_data, 
                                    lab = TRUE, 
                                    outline.color = "white",
                                    lab_size = 2, 
                                    colors = c("blue", "white", "red"),
                                    tl.col = "black",
                                    legend.title = "Correlation\ncoefficient")

# Save figure
ggsave(paste("../figures_cov/",params$data,"/Figure_S1_corrplot_",params$data,".png", sep = ""), 
       plot = predictor_correlation, 
       width = 25, height = 20, dpi = 300, units = "cm")
predictor_correlation
```


### DAPC
- We conduct Discriminant Analysis of Principal Components (DAPC), which is a two step method to first transform our data through Principal Component Analysis, and then conducts Discriminant Analysis on the retained principal components. This provides insight into linear combinations of our predictors that help explain the response variable. We use DAPC rather than Linear Discriminant Analysis since our predictors do not meet the requirements for LDA or normal distribution and no/minimal/comparable co-variance across classes.
- The confusion matrix from the 3 category DAPC is reported in the Supplementary Materials Table S5.
```{r}
# Format predictor dataset for analysis
explore_data_binary <- explore_data %>%
  filter(classification != 2) %>%
  droplevels()

predictor_data_binary <- explore_data_binary %>%
  select(-c(iso_code, year, classification))

# Conduct cross validation to determine number of PCs to include
png(here::here(fig_folder, paste("Figure_S2_xval_",params$data,".png", sep = "")), width = 30, height = 15, units = "cm", res = 300)
xval1 <- xvalDapc(predictor_data, grp = explore_data$classification, n.pca.max = 30, 
                  scale = TRUE, training.set = 0.7, n.da = 2, n.rep= 100,
                  result = "groupMean")
dev.off()
xval1

# Run DAPC - three country classifications
dapc1 <- dapc(predictor_data, grp = explore_data$classification,
              scale = TRUE, var.loadings = TRUE, n.pca = 28, n.da = 2,
              training.set = 0.7)
summary(dapc1)

# Confusion matrix
table(dapc1$grp, dapc1$assign)

# Plot DAPC - three country classifications
png(here::here(fig_folder, paste("Figure_3_DAPC_scatter_",params$data,".png", sep = "")), width = 30, height = 15, units = "cm", res = 300)

scatter(dapc1, scree.pca = TRUE, posi.pca = "bottomright", posi.da = "none",
        legend = TRUE, 
        txt.leg = c("Below expectations",
                    "Within expectations",
                    "Above expectations"),
        col = c(worse, within_ci, improved), 
        cstar = FALSE,
        solid = 0.7,
        clabel = 0,
        cex = 2)

dev.off()

# Illustrate key variables from DAPC with three country classifications
contrib1 <- loadingplot(dapc1$var.contr)

# Summarise performance compared to random
xval1_check <- xval1$`Cross-Validation Results` %>% filter(n.pca == max(n.pca)) %>%
  mutate(better = (success > xval1$`Median and Confidence Interval for Random Chance`[3]))

xval1_count <- xval1_check %>%
  count(better)
xval1_count
```

- Since it appears that the few datapoints for countries that performed above expectations may be skewing the findings (and that some of these may have had unreliable model fitting) we conduct a version of the DAPC analysis excluding these datapoints, and note the comparison of findings in the Results.
- We include the DAPC cross-validation of the 3 category model in the Supplementary Materials Figure S2. 
```{r}
# Conduct cross validation to determine number of PCs to include
xval2 <- xvalDapc(predictor_data_binary, grp = explore_data_binary$classification, 
                  n.pca.max = 100, 
                  scale = TRUE, training.set = 0.7, n.da =1, n.rep= 100,
                  result = "groupMean")
xval2

# Comparison Run DAPC - exclude countries that improved since seem to be skewing the data
dapc2 <- dapc(predictor_data_binary, grp = explore_data_binary$classification,
              scale = TRUE, var.loadings = TRUE, n.pca = 28, n.da = 1)
summary(dapc2)

# Plot comparison DAPC - two country categories
scatter(dapc2, scree.pca = TRUE, posi.pca = "bottomright", posi.da = "none",
        legend = TRUE, txt.leg = c("Below expected coverage", 
                                   "Within confidence intervals"),
        col = c(worse, within_ci))

# Illustrate key variables from DAPC with three country classifications
contrib2 <- loadingplot(dapc2$var.contr)

# Summarise performance compared to random
xval2_check <- xval2$`Cross-Validation Results` %>% filter(n.pca == max(n.pca)) %>%
  mutate(better = (success > xval2$`Median and Confidence Interval for Random Chance`[2]))

xval2_count <- xval2_check %>%
  count(better)
xval2_count 
```

## Random Forest
### Conduct random forest
```{r}
# Set seed for reproducibility
set.seed(42)

# Split data into train and test data
sample_size <- floor(0.7 * nrow(explore_data))
train_indices <- sample(seq_len(nrow(explore_data)), size = sample_size)
train_data <- explore_data[train_indices, ] 
test_data <- explore_data[-train_indices, ] 

# Remove descriptors for building random forest
train_dat <- train_data %>%
  select(-c("iso_code", "year"))

test_dat <- test_data %>%
  select(-c("iso_code", "year"))

# Work out how many trees to include in Random Forest
  # Train a Random Forest model with 1000 trees
  rf_ntree_test <- randomForest(classification ~ ., 
                          data = train_dat, 
                          ntree = 1000)
  
  # Plot error rate vs. number of trees
  plot(rf_ntree_test, main = "Error Rate vs. Number of Trees")
    # 200 trees seems sufficient  

# Work out how many variables to try at each split - sqrt(# variables) is default, i.e., 5.
mtry_values <- c(2, 5, 7, 10)
mtry_results <- data.frame(mtry = integer(), Accuracy = numeric())

# Loop through different mtry values and evaluate performance
for (m in mtry_values) {
  rf_model <- randomForest(classification ~ ., 
                           data = train_data,  
                           ntree = 150,  
                           mtry = m,  
                           importance = TRUE)
  predictions <- predict(rf_model, newdata = test_data)
  accuracy <- sum(predictions == test_data$classification) / nrow(test_data)
  mtry_results <- rbind(mtry_results, data.frame(mtry = m, Accuracy = accuracy))
}
mtry_results
  # 10 is best

# Train final model with the best parameters
# Random Forest
rf_model <- randomForest(classification ~ ., 
                         type = "classification", 
                         data = train_dat, 
                         ntree = 150, 
                         mtry = 10, 
                         importance = TRUE, 
                         rsq = TRUE)
rf_model
```
- We produce a figure to summarise the MDA and MDI scores per variable. The figure is produced as Figure S3 in the Supplementary Materials.
- Please note that there may be small differences between the results when this code is run, due to the Random Forest methodology. 
```{r}
# Review MDA and MDI of variables from model
importance(rf_model)
varImpPlot(rf_model)

# Output figure
importance_data <- importance(rf_model) %>%
  as.data.frame()

importance_data %<>%
  mutate(Variable = rownames(importance_data)) %>%
  select(Variable, `Mean Decrease in Accuracy (%)` = `MeanDecreaseAccuracy`, 
         `Mean Decrease in Impurity (%)` = `MeanDecreaseGini`) %>%
  mutate(Variable = recode(Variable, 
         "average_cov" = "Pre-pandemic DTP3 coverage",
         "ri_intros" = "Number of RI vaccines",
         "mean_doctors" = "Number of doctors",
         "mean_nurses" = "Number of nurses",
         "uhc_mean_index" = "Health system strength",
         "ghs_index" = "Global Health Security index",
         "econ_mort_excess" = "Excess mortality",
         "gov" = "Government health financing",
         "private" = "Private health financing",
         "ext" = "External (donor) health financing",
         "c1m_school" = "School closures",
         "c2m_work" = "Work closures",
         "c3m_events" = "Event cancellations",
         "c4m_gatherings" = "Gathering restrictions",
         "c5m_public_transit" = "Public transit closures",
         "c6m_stay_at_home" = "Stay at home orders",
         "c7m_internal_mov" = "Internal movement restrictions",
         "c8ev_international_travel" = "International travel restrictions",
         "e1_income_support" = "Income support",
         "e2_debt_relief" = "Debt relief",
         "h1_public_info" = "Public information",
         "h2_testing_policy" = "COVID-19 testing policy",
         "h3_contact_tracing" = "COVID-19 contact tracing",
         "h6m_masks" = "Mask wearing requirements",
         "h7_vx_policy" = "COVID-19 vaccine availability",
         "h8m_elderly" = "Elderly sheltering",
         "average_gdp" = "Gross Domestic Product",
         "population" = "Population size")) %>%
  arrange(-`Mean Decrease in Accuracy (%)`) %>%
  reshape2::melt(importance_df, id.vars = "Variable", 
                          measure.vars = c("Mean Decrease in Accuracy (%)", 
                                           "Mean Decrease in Impurity (%)"),
                          variable.name = "Metric", 
                          value.name = "Importance")

  # Create the plot
  importance_plot <- ggplot(importance_data, 
                            aes(x = reorder(Variable, Importance), 
                                y = Importance, fill = Metric)) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_text(aes(label = paste0(round(Importance, 2), "%")),  # Format values as percentages
              position = position_dodge(width = 0.9),  
              hjust = ifelse(importance_data$Importance < 0.015, -1, 1), 
              #nudge_y = ifelse(importance_data$Importance < 0, -1, 1),
              size =2.5) + 
    coord_flip() +  # Flip coordinates for better readability
    facet_wrap(~ Metric, scales = "free_x") +  # Separate plots for MDA and MDI
    labs(x = "Variables",
         y = "Importance Scores") +
    theme_minimal() +  
    scale_fill_manual(values = wes_palette("BottleRocket2", n = 2)) +
    theme(axis.title.x = element_text(size = 10, face = "bold"),
          axis.title.y = element_text(size = 10, face = "bold"),
          axis.text = element_text(size = 8),
          plot.title = element_text(size = 12, face = "bold"),
          legend.position = "none", 
          strip.text = element_text(size = 8, face = "bold"),
          panel.background = element_rect(fill = "white", colour = "white"),
        plot.background = element_rect(fill = "white", colour = "white"))
  importance_plot

  # Save plot  
  ggsave(importance_plot, 
         filename = paste("../figures_cov/",params$data,"/Figure_S3_rf_importance.png", sep =""),
         height = 10, width = 25, units = "cm")
```

### Evaluate the performance of the model
- We evaluate the overall predictive power of the model by evaluating the test dataset using the resulting Random Forest model
- The confusion matrix is reported in the Supplementary Materials Table S6.
```{r}
# Predictive power
  # Compare performance of using Random Forest model as predictor on test dataset
  predictions <- predict(rf_model, newdata = test_dat)
  
  # Evaluate the model performance
  actuals <- test_dat$classification
  
# Compare results
  comparison <- test_data %>%
    select(iso_code, classification) %>%
    cbind(predictions) %>%
    mutate(accuracy = case_when(predictions == classification ~ 1,
                                .default = 0))
  
  rf_confusion_matrix <- confusionMatrix(comparison$predictions, comparison$classification)
  rf_confusion_matrix
```

# CONSOLIDATED COUNTRY-LEVEL RESULTS 

## Create summary output csv with all country-level data
```{r}
output_res <- res_all %>%
  select(country, iso_code, region, income_group, year, arima_model = method,
         forecast_coverage = mean, lower_ci, upper_ci, reported_coverage = coverage,
         delta, lower_delta, upper_delta, within_ci, vaxxed) %>%
  mutate(year = as.numeric(year))

si_long <- unwpp_all %>%
  select(iso_code, year, "surviving_infants") %>%
  filter(year > 2019) 
  
output_res %<>%
  left_join(si_long, by = c("iso_code", "year"))

write.csv(
  output_res,
  here::here(csv_folder, paste("Supplementary_table_",params$data,".csv", sep = "")),
  row.names = FALSE)
```
